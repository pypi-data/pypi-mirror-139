"""
Provides the migration mechanics for the PostgreSQL database.

This facility makes possible to read the current database state, analysing
system tables with description of all tables and all columns, indexes, etc.
After that, it makes possible to understand whose database tables fully
corresponds to the declared ORM models, whose are not.

This facility generates necessary SQL statements targeted to make the
PostgreSQL database schema to be identical to the declared ORM schema.

This avoids to handle migration history (like traditional ORMs
usually provides), but take in the account the current state, differ
to the ORM's declared schema, understand what must be done to make the
database tables fully identical to the declared schema and execute
necessary SQL code.

The migration facility of the Wefram does not stores any migration
history or SQL statements. It is not needed to. Any changes made to
the ORM schema in the project will be transfered to the database,
making the database identical to the CURRENT state, to the NOW.
Not matter what will happen next and whose model will be declared or
modified.
"""

from typing import *
from datetime import date, datetime
from dataclasses import dataclass

import sqlalchemy.exc
from sqlalchemy import text
from sqlalchemy.sql import schema
from . import engine, model
from ... import config, logger
from ...tools import CSTYLE


@dataclass
class PgColumn:
    name: str
    nullable: bool
    data_type: str
    column_type: str
    default: Any
    is_identity: bool
    identity_start: Union[None, int]
    identity_increment: Union[None, int]
    identity_cycle: bool

    def default_sql(self) -> str:
        if self.is_identity:
            return "GENERATED BY DEFAULT AS IDENTITY" \
                   f" (INCREMENT BY {self.identity_increment} START WITH {self.identity_start}" \
                   f"{' CYCLE' if self.identity_cycle else ''})"
        return f'DEFAULT {str(self.default)}'

    def create_sql(self) -> str:
        coldef: List[str] = [
            f'"{self.name}"',
            self.data_type
        ]
        if not self.nullable:
            coldef.append('NOT NULL')
        if self.default is not None or self.is_identity:
            coldef.append(self.default_sql())
        return ' '.join(coldef)


@dataclass
class PgKey:
    name: str
    constraint_name: str
    key_type: str
    is_foreign_key: bool
    referenced_table_name: Union[str, None]
    referenced_columns: List[str]
    on_update: Union[None, str]
    on_delete: Union[None, str]
    col_aliases: List[str]


@dataclass
class PgForeignKey:
    constraint_name: str
    model_idx_columns: List[str]
    ref_idx_columns: List[str]
    ref_table: str
    on_update: Union[None, str]
    on_delete: Union[None, str]


@dataclass
class PgUniqueKey:
    constraint_name: str
    col_aliases: List[str]


@dataclass
class PgIndexKey:
    key_name: str
    col_aliases: List[str]
    column_names: List[str]


@dataclass
class PgTable:
    keys: Dict[str, PgKey]
    columns: Dict[str, PgColumn]
    indexes: Dict[str, PgIndexKey]
    foreign_keys: Dict[str, PgForeignKey]
    unique_keys: Dict[str, PgUniqueKey]
    primary_key: List[str]
    pk_keynames: List[str]


class DatabaseMigration:
    def __init__(
            self,
            pg_schema: str = 'public',
            drop_missing_columns: bool = True,
            drop_missing_tables: bool = True
    ):
        self.cn = None
        self.schema: str = pg_schema
        self.drop_missing_columns: bool = bool(drop_missing_columns)
        self.drop_missing_tables: bool = bool(drop_missing_tables)
        self.current_tables: Dict[str, PgTable] = {}
        self.declared_tables: List[schema.Table] = []
        self.mapped_tables: Dict[str, schema.Table] = {}

    async def migrate(self):
        async with engine.engine.connect() as cn:
            self.cn = cn
            self.current_tables = await self.get_tables_current()
            self.declared_tables = model.DatabaseModel.metadata.sorted_tables
            self.mapped_tables: Dict[str, schema.Table] = {
                t.name: t for t in self.declared_tables
            }

            migrate_arr: List[str] = []

            for t in self.declared_tables:
                tn: str = t.name
                q: Optional[str] = self.alter_table_sql(t) \
                    if tn in self.mapped_tables \
                    else self.create_table_sql(t)
                if not q:
                    continue
                migrate_arr.append(q)

            if self.drop_missing_tables:
                for tn in self.current_tables.keys():
                    if tn in self.mapped_tables.keys():
                        continue
                    migrate_arr.append(f'DROP TABLE "{tn}"')

            if migrate_arr:
                await self.deconstraint()
                [await self.execute(q, log=True) for q in migrate_arr]
                await self.reconstraint()

            await cn.commit()

    async def execute(self, sql: str, params: Optional[dict] = None, log: bool = False):
        if log:
            logger.debug(f"{CSTYLE['bold']}migrate database{CSTYLE['clear']}: executing:\n{sql}")
        params = params if isinstance(params, dict) else {}
        return await self.cn.execute(text(sql), params)

    async def query(self, sql: str, params: Optional[dict] = None):
        return (await self.execute(sql, params)).fetchall()

    async def get_tables_current(self) -> Dict[str, PgTable]:
        tablenames: List[str] = await self.get_tablenames_current()
        return {tn: await self.get_table_current(tn) for tn in tablenames}

    async def get_tablenames_current(self) -> List[str]:
        sql: str = 'SELECT "table_name" AS table_name FROM "information_schema"."tables" WHERE "table_schema" = :ts'
        return [r['table_name'] for r in (await self.query(sql, {'ts': self.schema}))]

    async def get_table_current(self, tablename: str) -> Optional[PgTable]:
        """ Reads the table structure from DBMS of the Model. Reads attributes definitions and keys,
        including indexes, primary keys, foreign keys, unique keys, etc.

        :param tablename:
            The existing database table name.
        :type tablename:
            str

        :returns:
            The existing database table structure.
        :rtype:
            dict
        """

        table: PgTable = PgTable(
            keys={},
            columns={},
            indexes={},
            foreign_keys={},
            unique_keys={},
            primary_key=[],
            pk_keynames=[]
        )

        query: str
        dbr: list

        # Selecting attributes definitions from INFORMATION_SCHEMA, basing on PostgreSQL documentation
        # "34.15. The Information Schema.columns"
        query = (
            'SELECT "column_name", "column_default", "is_nullable", "data_type", "character_maximum_length",'
            ' "character_octet_length", "numeric_precision", "numeric_scale", "datetime_precision", "udt_name",'
            ' "is_identity", "identity_start", "identity_increment", "identity_cycle"'
            ' FROM "information_schema"."columns" WHERE "table_schema"=:ts AND "table_name"=:tn'
        )
        dbr = (await self.query(query, {'ts': self.schema, 'tn': tablename}))
        if not dbr:
            return None

        for r in dbr:
            column_name: str = r['column_name']
            column_def: PgColumn = PgColumn(
                name=column_name,
                nullable=True if str(r['is_nullable']).upper() == "YES" else False,
                data_type=str(r['data_type']).lower(),
                column_type=str(r['udt_name']).upper(),
                default=r['column_default'],
                is_identity=str(r['is_identity']).upper() == 'YES',
                identity_start=int(r['identity_start']) if r['identity_start'] else None,
                identity_increment=int(r['identity_increment']) if r['identity_increment'] else None,
                identity_cycle=str(r['identity_cycle']).upper() == 'YES',
            )
            if column_def.data_type == 'character varying':
                character_maximum_length: Optional[int] = r['character_maximum_length']
                if character_maximum_length:
                    column_def.data_type += f"({r['character_maximum_length']})"
            elif column_def.data_type == 'numeric':
                numeric_precision: Optional[int] = r['numeric_precision']
                numeric_scale: Optional[int] = r['numeric_scale']
                numeric: List[str] = []
                if numeric_precision:
                    numeric.append(str(numeric_precision))
                if numeric_scale:
                    numeric.append(str(numeric_scale))
                if numeric:
                    column_def.data_type += f"({', '.join(numeric)})"

            table.columns[column_name] = column_def

        # Selecting informations about declared keys usage, basing on PostgreSQL documentation
        # "34.30 The Information Schema.key_column_usage"
        query = (
            'SELECT tc."constraint_name" AS constraint_name,'
            ' kcu."column_name" AS column_name,'
            ' tc."constraint_type" AS constraint_type'
            ' FROM "information_schema"."table_constraints" tc'
            ' JOIN "information_schema"."key_column_usage" kcu'
            ' ON kcu."constraint_name" = tc."constraint_name"'
            ' WHERE tc."constraint_schema"=:ts AND tc."table_name"=:tn'
            ' AND UPPER(tc."constraint_type") IN (\'PRIMARY KEY\', \'FOREIGN KEY\', \'UNIQUE\')'
            ' ORDER BY kcu."constraint_name", kcu."position_in_unique_constraint", kcu."ordinal_position"'
        )
        for r in (await self.query(query, {'ts': self.schema, 'tn': tablename})):
            key_name: str = str(r['constraint_name'])
            key_type: str = str(r['constraint_type']).upper()
            if key_type == 'PRIMARY KEY' and r['column_name']:
                # PRIMARY KEY part
                table.primary_key.append(r['column_name'])
                table.pk_keynames.append(r['constraint_name'])
                continue
            if key_name in table.keys:
                # Composite key part
                table.keys[key_name].col_aliases.append(r['column_name'])
                continue
            # Defining a new key
            table.keys[key_name] = PgKey(
                name=key_name,
                constraint_name=key_name,
                key_type=key_type,
                is_foreign_key=(key_type == "FOREIGN KEY"),
                referenced_table_name=None,
                referenced_columns=[],
                on_update=None,
                on_delete=None,
                col_aliases=[r['column_name'], ]
            )

        # Additionally selecting information about referential constraints - foreign keys. Doing it
        # by selecting from REFERENTIAL_CONSTRAINTS basing on PostgreSQL documentation
        # "34.32 The Information Schema.referential_constraints"
        # Note that PostgreSQL gives no posibility to filter referential_constraints table againts
        # specific table name and returns all constraints for the database at a time.
        query = (
            'SELECT "constraint_name", "update_rule", "delete_rule" FROM'
            ' "information_schema"."referential_constraints" WHERE'
            ' "constraint_schema" = :ts'
        )
        for r in (await self.query(query, {'ts': self.schema})):
            fk_name: str = r['constraint_name']
            if fk_name not in table.keys:
                continue
            table.keys[fk_name].is_foreign_key = True
            table.keys[fk_name].on_update = r['update_rule'] or None
            table.keys[fk_name].on_delete = r['delete_rule'] or None

        # Handling got keys
        for name in table.keys:
            # We about to define three general types of keys:
            # a) PRIMARY KEY
            # b) FOREIGN KEY
            # c) UNIQUE KEY
            # INDEX KEY (or just a KEY in terms of PostgreSQL) are stored
            # separatelly, so will not be found in general KEYS structure.
            # The first type (a) - the PRIMARY KEY (even composite)
            # detects in the procedure above and not stored in the general
            # `keys` variable, so this is not a point to work with PK here.
            key: PgKey = table.keys[name]
            is_fk: bool = key.is_foreign_key
            if is_fk:
                # Due PostgreSQL does not stores information about foreign key referenced
                # table and columns in the information schema with key column usage - we have
                # to query for this information additionaly againts every FK constraint.
                query = (
                    'SELECT "table_name", "column_name"'
                    ' FROM "information_schema"."constraint_column_usage"'
                    ' WHERE "constraint_name"=:cn'
                )
                dbr = (await self.query(query, {'cn': key.constraint_name}))
                if dbr:
                    ref_tablename: Optional[str] = None
                    ref_columns: List[str] = []
                    for r in dbr:
                        ref_tablename = str(r['table_name'])
                        ref_columns.append(str(r['column_name']))
                    key.referenced_table_name = ref_tablename
                    key.referenced_columns = ref_columns
                table.foreign_keys[name] = PgForeignKey(
                    constraint_name=key.constraint_name,
                    model_idx_columns=key.col_aliases,
                    ref_idx_columns=key.referenced_columns,
                    ref_table=key.referenced_table_name,
                    on_update=key.on_update,
                    on_delete=key.on_delete
                )
            else:
                table.unique_keys[name] = PgUniqueKey(
                    constraint_name=key.constraint_name,
                    col_aliases=key.col_aliases
                )

        # Selecting information about INDEX KEYs, basing on internal PostgreSQL functionality.
        # Too bad, but this information is not accessible from standard schema (from
        # INFORMATION SCHEMA), so we have to use PostgreSQL specific way only.
        query = (
            "SELECT ix.relname AS relname,"
            " regexp_replace(pg_get_indexdef(indexrelid), '.*\\((.*)\\)', '\\1') AS idx_columns"
            " FROM pg_index i"
            " JOIN pg_class t ON t.oid = i.indrelid"
            " JOIN pg_class ix ON ix.oid = i.indexrelid"
            " WHERE t.relname = :tn"
        )
        for r in (await self.query(query, {'tn': tablename})):
            name: str = r['relname']
            # If the index key is already registered in the `keys` varitable - skipping
            # due this is not an index key.
            if name in table.keys or name in table.pk_keynames:
                continue
            table.indexes[name] = PgIndexKey(
                key_name=name,
                col_aliases=[],
                column_names=[]
            )
            idx_columns: List[str] = r['idx_columns'].split(',')
            for cn in idx_columns:
                table.indexes[name].col_aliases.append(cn)
                table.indexes[name].column_names.append(cn)

        # Accumulating the resulting information about model's table definition
        return table

    async def deconstraint(self) -> None:
        for tn, table in self.current_tables.items():
            constraints: list = \
                list(table.foreign_keys.keys()) \
                + list(table.unique_keys.keys())
            for cn in constraints:
                await self.execute(
                    f'ALTER TABLE "{tn}" DROP CONSTRAINT IF EXISTS "{cn}" CASCADE',
                )

    async def reconstraint(self) -> None:
        for table in self.declared_tables:
            tn: str = table.name

            # Foreign keys
            fkcs: Set[schema.ForeignKeyConstraint] = table.foreign_key_constraints
            if not fkcs:
                continue
            for fkc in fkcs:
                fks: List[schema.ForeignKey] = fkc.elements
                fk_columns: List[schema.Column] = fkc.columns
                fk_refcolumns: List[schema.Column] = [
                    fk.column for fk in fks
                ]
                rt: schema.Table = fkc.referred_table
                rtn: str = rt.name
                cn: str = '_'.join([tn] + [c.name for c in fk_columns] + ['fkey'])
                q_columns: str = ','.join([
                    f'"{c.name}"' for c in fk_columns
                ])
                q_refcolumns: str = ','.join([
                    f'"{c.name}"' for c in fk_refcolumns
                ])
                q_rulesarr: List[str] = []
                if fkc.ondelete:
                    q_rulesarr.append(f"ON DELETE {fkc.ondelete}")
                if fkc.onupdate:
                    q_rulesarr.append(f"ON UPDATE {fkc.onupdate}")
                q_rules: str = ' '.join(q_rulesarr)
                await self.execute(
                    f'ALTER TABLE "{tn}" ADD CONSTRAINT "{cn}"'
                    f' FOREIGN KEY ({q_columns})'
                    f' REFERENCES "{rtn}" ({q_refcolumns}) {q_rules}'
                )

            # Unique keys
            ucs: List[schema.UniqueConstraint] = [
                c for c in table.constraints if isinstance(c, schema.UniqueConstraint)
            ]
            for uc in ucs:
                cn: str = '_'.join([tn] + [c.name for c in uc.columns] + ['key'])
                cns: str = ','.join(f'"{c.name}"' for c in uc.columns)
                await self.execute(
                    f'ALTER TABLE "{tn}" ADD CONSTRAINT "{cn}"'
                    f' UNIQUE ({cns})'
                )

    def create_table_sql(self, table: schema.Table) -> str:
        tn: str = table.name
        qc: List[str] = [self.column_create_sql(c) for c in table.columns]
        pk: schema.PrimaryKeyConstraint = table.primary_key
        pk_columns: str = ','.join([f'"{c.name}"' for c in pk.columns])
        qi: str = ',\n'.join(
            [f"  {c}" for c in qc]
            + [f'  PRIMARY KEY ({pk_columns})']
        )
        return f'CREATE TABLE "{tn}" (\n{qi}\n)'

    def alter_table_sql(self, table: schema.Table) -> Optional[str]:
        tn: str = table.name
        current: PgTable = self.current_tables.get(tn, None)
        if not current:
            return self.create_table_sql(table)

        mapped_columnnames: List[str] = []
        alter_arr: List[str] = []
        for c in table.columns:
            cn: str = c.name
            mapped_columnnames.append(cn)

            if cn in current.columns:
                cc: PgColumn = current.columns[cn]
                declared_create_sql: str = self.column_create_sql(c)
                current_create_sql: str = cc.create_sql()
                if declared_create_sql == current_create_sql:
                    continue
                alter_arr.append(self.column_alter_sql(c))
            else:
                alter_arr.append(f'ADD COLUMN {self.column_create_sql(c)}')

        if self.drop_missing_columns:
            for cn in current.columns.keys():
                if cn in mapped_columnnames:
                    continue
                alter_arr.append(f'DROP COLUMN "{cn}"')

        if alter_arr:
            return f'ALTER TABLE "{tn}"\n  ' + ',\n  '.join(alter_arr)

    def column_create_sql(self, c: schema.Column) -> str:
        coldef: List[str] = [
            f'"{c.name}"',
            self.column_sql_type(c)
        ]

        if not c.nullable:
            coldef.append('NOT NULL')

        column_default: Any = self.column_default(c)
        if column_default is not None:
            if isinstance(column_default, str) and 'IDENTITY' in column_default:
                coldef.append(column_default)
            else:
                coldef.append(f'DEFAULT {column_default}')

        return ' '.join(coldef)

    def column_alter_sql(self, c: schema.Column) -> str:
        if c.identity is not None:
            raise NotImplementedError(
                "Automatic modifying the existing IDENTITY columns (auto-increment primary keyed columns)"
                " has not been implemented yet! Do NOT alter primary key columns!!!"
            )

        column_default: Any = self.column_default(c)

        alters: List[str] = list()
        alters.append(f'TYPE {self.column_sql_type(c)}')
        if c.primary_key and c.identity is not None:
            pass  # TODO
        elif column_default is None:
            alters.append(f'DROP DEFAULT')
        else:
            alters.append(f'SET DEFAULT {column_default}')
        alters.append('SET NOT NULL' if not c.nullable else 'DROP NOT NULL')

        return ', '.join([
            f'ALTER COLUMN "{c.name}" {a}' for a in alters
        ])

    def column_default(self, c: schema.Column) -> Any:
        if c.primary_key and c.identity is not None:
            # If this column is a primary key and serial (auto-increment)
            identity: schema.Identity = c.identity
            return "GENERATED BY DEFAULT AS IDENTITY" \
                   f" (INCREMENT BY {identity.increment} START WITH {identity.start}"\
                   f"{' CYCLE' if identity.cycle else ''})"

        declared_default: Optional[schema.ColumnDefault] = c.default
        if declared_default is None:
            return None

        default_arg: Any = declared_default.arg

        if declared_default.is_callable:
            # If the default value of the column is callable function
            if default_arg == datetime.now:
                # If the callable is NOW - we can set if on the RDBMS side
                return "now()"
            if default_arg == date.today:
                # If the callable is TODAY - we can set if on the RDBMS side
                return "CURRENT_DATE"
            return None

        if declared_default.is_scalar:
            value: Any = declared_default.arg
            if isinstance(value, bool):
                return 'true' if value else 'false'
            if isinstance(value, str):
                return "'" + str(value).replace("'", "\\'") + "'::character varying"
            return declared_default.arg

        return None

    def column_sql_type(self, c: schema.Column) -> str:
        typedef: str = str(c.type).lower()

        if typedef.startswith('varchar'):
            typedef = typedef.replace('varchar', 'character varying')

        elif typedef == 'datetime':
            typedef = 'timestamp'
            if c.type.timezone:
                typedef += ' with time zone'
            else:
                typedef += ' without time zone'

        return typedef

    async def drop_all(self) -> None:
        async with engine.engine.connect() as cn:
            self.cn = cn
            self.current_tables = await self.get_tables_current()
            await self.deconstraint()
            [await self.execute(
                f'DROP TABLE "{tn}"', log=True
            ) for tn in self.current_tables.keys()]
            await cn.commit()


async def migrate() -> None:
    """ Starts the migration process, bringing the state of the database to that which was declared
    for the project.
    """
    await (DatabaseMigration(
        drop_missing_tables=bool(config.DATABASE.get('migrate', {}).get('drop_missing_tables')),
        drop_missing_columns=bool(config.DATABASE.get('migrate', {}).get('drop_missing_columns'))
    )).migrate()


async def dropall() -> None:
    """ Drops all data in the corresponding, specified in the config, database. """
    await (DatabaseMigration()).drop_all()

