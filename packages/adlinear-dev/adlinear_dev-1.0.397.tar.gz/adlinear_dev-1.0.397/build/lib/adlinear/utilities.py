import math
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats

# import typing
from typing import Union, Tuple
from sklearn.cluster import KMeans
from sklearn import metrics
# from . import clusterizer as cls


def hhi_1(tab) -> float:
    tab = [abs(x) for x in tab]
    listsum = sum(tab)
    if listsum == 0:
        return 0
    else:
        rtab = [x / listsum for x in tab]
        tab2 = [x * x for x in rtab]
        return 1.0 / sum(tab2)


def mat_hhi_1(mat, axis=0) -> pd.DataFrame:
    mat = pd.DataFrame(mat)
    if axis == 0:
        res = pd.DataFrame(index=mat.columns, columns=["InvHHI"])
        res.loc[:, "InvHHI"] = [hhi_1(mat.loc[:, col]) for col in mat.columns]
    else:
        res = pd.DataFrame(index=mat.index, columns=["InvHHI"])
        res.loc[:, "InvHHI"] = [hhi_1(mat.loc[row, :]) for row in mat.index]
    return res


def count_not_nans(mat) -> int:
    return np.count_nonzero(~np.isnan(mat.astype(float)))


def count_nans(mat) -> int:
    return np.count_nonzero(np.isnan(mat.astype(float)))


def mean_square(mat) -> float:
    sz = count_not_nans(mat)
    if sz == 0:
        return np.nan
    else:
        mat = np.nan_to_num(mat, nan=0.0)
        return np.sum(mat ** 2) / sz


def norm_n2(vec) -> float:
    vec = np.nan_to_num(vec, nan=0.0)
    return (np.sum(vec**2))**0.5


def relative_squared_error(mat1, mat2, refmat=None) -> float:
    assert mat1.shape == mat2.shape
    if refmat is None:
        meansq1 = mean_square(mat1)
        meansq2 = mean_square(mat2)
        meansq = 0.5 * (meansq1 + meansq2)
    else:
        meansq = mean_square(refmat)
    return mean_square(mat1 - mat2) / meansq if meansq > 0 else 1e9


def freq_bins(mat, nbins=10) -> Union[tuple, np.ndarray, float]:
    """

     freq_bins:

     Parameters
     ----------

     mat: a float array or matrix

     nbins: a positive integer representing a linear binning of the values

     :returns:
     a DataFrame containing the features (attributes) as columns, the observations (e.g stocks) as lines
     """

    values = pd.DataFrame(np.ravel(mat)).dropna()
    return scipy.stats.relfreq(values, nbins)


def normalized_entropy(mat, nbins=3) -> float:
    assert nbins > 0
    if nbins == 1:
        return 0
    else:
        freqs = freq_bins(mat, nbins)
        return scipy.stats.entropy(freqs.frequency) / math.log(nbins)


def relative_entropy_by_cluster(mat, lineclusters, colclusters, nbins=10):
    assert nbins > 0
    nl, nc = mat.shape
    assert nl == len(lineclusters)
    assert nc == len(colclusters)
    l_clust_values = pd.unique(pd.DataFrame(lineclusters).iloc[:, 0].sort_values())
    c_clust_values = pd.unique(pd.DataFrame(colclusters).iloc[:, 0].sort_values())

    clust_values = list(set(l_clust_values).intersection(c_clust_values))
    df_entropies = pd.DataFrame(index=clust_values, columns=clust_values)
    df_sizes = pd.DataFrame(index=clust_values, columns=clust_values)
    for l_val in clust_values:
        for c_val in clust_values:
            val_lines = lineclusters[lineclusters == l_val].dropna().index
            val_cols = colclusters[colclusters == c_val].dropna().index
            submat = mat.loc[val_lines, val_cols]
            df_entropies.loc[l_val, c_val] = normalized_entropy(submat, nbins)
            df_sizes.loc[l_val, c_val] = len(val_lines) * len(val_cols)
    return df_entropies, df_sizes


def clustering_caha_score(mat, clusters):
    return metrics.calinski_harabasz_score(mat, clusters)


def clustering_caha_score_by_ncomp(mat, clustermodel, ncompmin=2, ncompmax=20):
    kmeans_model = clustermodel.fit(mat)
    return metrics.calinski_harabasz_score(mat, clustermodel)


def censor_data(mat, p: float = 0.0, censored_indices: Tuple[int] = [], inplace=True, randomgen=None):
    """
    Parameters
    ----------
    censored_indices : object

    """
    assert (0.0 <= p < 1.0)
    if p == 0.0 and len(censored_indices) == 0:
        return mat.copy() if not inplace else None
    else:
        if randomgen is None:
            randomgen = np.random.default_rng()
        nr = len(mat.index)
        matc = mat if inplace else mat.copy()
        if len(censored_indices) > 0:
            for idx in censored_indices:
                matc.iloc[idx[0], idx[1]] = np.nan
        else:
            nans = int(abs(p) * nr)
            for icol, _ in enumerate(matc.columns):
                # censored_idx = np.random.Generator.choice(range(nr), nans, replace=False)
                censored_idx = randomgen.choice(range(nr), nans, replace=False)
                matc.iloc[censored_idx, icol] = np.nan

        return None if inplace else matc


def swap_in_mat(mat, i, j, axis=0, inplace=True):
    """
    swap_in_mat: swaps two lines or two rows of a matrix.
    Can be in place or not.

    :returns
    None if in place, swapped matrix otherwise
    """
    nr, nc = mat.shape
    assert (0 <= i < nr and 0 <= j < nc)
    if i == j:
        return None if inplace else mat
    else:
        matc = mat if inplace else mat.copy()
        chunk_i = mat.iloc[i, :] if axis == 0 else mat.iloc[:, i]
        chunk_j = mat.iloc[j, :] if axis == 0 else mat.iloc[:, j]
        temp = chunk_i.copy()
        if axis == 0:
            matc.iloc[i, :] = chunk_j
            matc.iloc[j, :] = temp
        else:
            matc.iloc[:, i] = chunk_j
            matc.iloc[:, j] = temp

        return None if inplace else matc


def make_diagonal_dominant(mat, inplace=True):
    nr, nc = mat.shape
    assert (nr == nc)
    matc = mat if inplace else mat.copy()
    for ir, _ in enumerate(mat.index):
        cmax = np.argmax(matc.iloc[ir, :])
        swap_in_mat(matc, ir, cmax, axis=1, inplace=True)
    return None if inplace else matc


def get_observations_km_clusters(mat, ngroups=2):

    km = KMeans(n_clusters=ngroups)
    return km.fit_predict(mat)


def get_clustering_incidence_matrix(df_clust1: Union[np.ndarray, pd.DataFrame, Tuple[int]],
                                    df_clust2: Union[np.ndarray, pd.DataFrame, Tuple[int]]):

    # assert (len(mat) == len(df_clust1) and len(df_clust1) == len(df_clust2))
    assert len(df_clust1) == len(df_clust2)
    nobs = len(df_clust1)
    df_clust1 -= np.min(df_clust1) - 1
    df_clust2 -= np.min(df_clust2) - 1
    incidence_mat = pd.DataFrame(index=sorted(pd.unique(df_clust1.iloc[:, 0])),
                                 columns=sorted(pd.unique(df_clust2.iloc[:, 0]))).fillna(0.0, inplace=False)
    df_clust1 = pd.DataFrame(df_clust1)
    df_clust2 = pd.DataFrame(df_clust2)
    for iobs in range(nobs):
        clust1 = int(df_clust1.iloc[iobs])
        clust2 = int(df_clust2.iloc[iobs])
        incidence_mat.loc[clust1, clust2] = incidence_mat.loc[clust1, clust2] + 1

    return incidence_mat


def get_clustering_mismatches(df_clust1: Union[np.ndarray, pd.DataFrame, Tuple[int]],
                              df_clust2: Union[np.ndarray, pd.DataFrame, Tuple[int]]):
    assert len(df_clust1) == len(df_clust2)
    nobs = len(df_clust1)
    inc_mat = get_clustering_incidence_matrix(df_clust1, df_clust2)
    make_diagonal_dominant(inc_mat, inplace=True)
    nobj = len(nobs)
    nmis = nobj - np.trace(inc_mat)
    return nmis, float(nmis) / nobj


def get_nb_misclassifieds_old(mat, grp_priors):
    ngrps = len(set(grp_priors))
    km_grps = get_observations_km_clusters(mat, ngrps)
    return get_clustering_mismatches(km_grps, grp_priors)


def get_nb_misclassifieds(mat: Union[np.ndarray, pd.DataFrame],
                          cltzr,
                          ref_cltzr):
    ngrps = cltzr.get_nbgroups()
    ref_ngrps = ref_cltzr.get_nbgroups()
    assert(ngrps == ref_ngrps)
    grps = cltzr.apply(mat)
    ref_grps = ref_cltzr.apply(mat)
    return get_clustering_mismatches(mat, grps, ref_grps)


def get_avg_features_by_clusters(mat: Union[np.ndarray, pd.DataFrame], cltzr):
    groups = cltzr.apply(mat)
    ngroups = len(set(groups))
    df_mat = pd.DataFrame(mat)
    df_means = pd.DataFrame(index=range(ngroups), columns=df_mat.columns)
    df_mat['cluster'] = groups
    for grp in range(ngroups):
        submat = df_mat[df_mat['cluster'] == grp]
        df_means.loc[grp, :] = np.nanmean(submat.drop(['cluster'], axis=1), axis=0)
    return df_means, groups


def get_avg_features_by_km_clusters(mat: Union[np.ndarray, pd.DataFrame], ngroups=2):
    km = get_observations_km_clusters(mat, ngroups)
    df_mat = pd.DataFrame(mat)
    df_means = pd.DataFrame(index=range(ngroups), columns=df_mat.columns)
    df_mat['cluster'] = km
    for grp in range(ngroups):
        submat = df_mat[df_mat['cluster'] == grp]
        df_means.loc[grp, :] = np.nanmean(submat.drop(['cluster'], axis=1), axis=0)
    return df_means, km


def fill_missing(mat, method="median", axis=0, fill_values=None, ngroups=0):

    if type(mat) is not pd.DataFrame:
        cols = list(["F" + str(x) for x in range(1, mat.shape[1] + 1)])
        filled_mat = pd.DataFrame(data=mat.values, columns=cols)
    else:
        filled_mat = mat.copy().astype(float)

    if method in ["median", "mean"]:
        if method == "median":
            df_estimates = mat.astype(float).apply(np.nanmedian, axis=axis)
        elif method == "mean":
            df_estimates = mat.astype(float).apply(np.nanmean, axis=axis)
        if axis == 0:
            # TODO: do the fill in next step without for loop
            # for col in mat.columns:
            #     filled_mat[col].fillna(value=df_estimates[col], inplace=True)
            filled_mat.fillna(value=df_estimates.to_dict(), inplace=True)
        else:
            for idx in mat.index:
                filled_mat.loc[idx].fillna(value=df_estimates.loc[idx], inplace=True)

    elif method == "kmeans":
        # KMeans ne tolÃ¨re pas les valeurs manquantes: imputer la moyenne par feature
        km_filled_mat = fill_missing(mat, method="mean", axis=axis)
        df_means, km = get_avg_features_by_km_clusters(km_filled_mat, ngroups)
        # reprendre la matrice incomplÃ¨te
        filled_mat = mat.copy()
        for idx in filled_mat.index:
            grp = km[idx]
            filled_mat.loc[idx] = mat.loc[idx].fillna(value=df_means.loc[grp, :])

    elif method == "values" and fill_values is not None:
        # update works inplace
        filled_mat.update(other=fill_values, overwrite=False)

    return filled_mat


def shorten_features_names(mat, wished_len=20):
    if hasattr(mat, "columns"):
        cols_dict = dict.fromkeys(list(mat.columns))
        for col in cols_dict:
            cols_dict[col] = col[0:wished_len] if len(col) >= wished_len else col
        mat = mat.rename(columns=cols_dict)
    return mat


def center_by_columns(mat):
    if type(mat) is pd.DataFrame:
        pass
    else:
        cols = list(["F" + str(x) for x in range(1, mat.shape[1] + 1)])
        mat = pd.DataFrame(data=mat.values, columns=cols)
    centered_mat = mat.copy()
    return centered_mat


def normalize_by_columns(mat: Union[np.ndarray, pd.DataFrame],
                         make_positive: bool = True,
                         add_cst: Union[None, np.ndarray, pd.DataFrame] = None,
                         mult_cst: Union[None, np.ndarray, pd.DataFrame] = None):
    """
    normalize_by_columns: applies linear transformation to a matrix mat
    mat <- (mat - additive_constant_vector) / multiplicative_constant_vector
    The operations are done columnwise
    Parameters
    ----------
    mat: Union[np.ndarray, pd.DataFrame]: the matrix to normalize
    make_positive: bool
        if true, the constant vector is the minimum by columns
        if false, the mean by colums unless the parameter add_cst is not None
    add_cst:
        a forced additive constant. If none, the mean columnwise.
    mult_cst:
        a forced multiplicative constant. If None, the stdev columnwise.

    Returns
    -------
    The normalized matrix
    The additive constant effectively used
    The multiplicative constant effectively used
    """
    if type(mat) is not pd.DataFrame:
        cols = list(["F" + str(x) for x in range(1, mat.shape[1] + 1)])
        mat = pd.DataFrame(data=mat, columns=cols)
    normalized_mat = mat.copy()
    take_min = make_positive and (add_cst is None)
    add_constants = np.nanmin(normalized_mat, axis=0) if take_min else \
        add_cst if (add_cst is not None) else np.nanmean(normalized_mat, axis=0)
    mult_constants = mult_cst if (mult_cst is not None) else np.nanstd(normalized_mat, axis=0)
    normalized_mat = np.subtract(normalized_mat, add_constants.T)
    normalized_mat = np.divide(normalized_mat, mult_constants.T)

    return normalized_mat, add_constants, mult_constants


def denormalize_by_columns(mat, means=0, stdevs=1):
    if type(mat) is not pd.DataFrame:
        cols = list(["F" + str(x) for x in range(1, mat.shape[1] + 1)])
        mat = pd.DataFrame(data=mat.values, columns=cols)
    denormalized_mat = mat.copy()
    denormalized_mat *= stdevs
    denormalized_mat += means

    return denormalized_mat


def sort_columns_by_fill_rate(mat, grouping_vect=None):
    """

    - si grouping_vect == [], on calcule le taux de remplissage sur la totalitÃ© de la matrice
    - si grouping_vect n'est pas vide, c'est un DF indexÃ© comme mat (par des ids d'objets)
    et le taux de remplissage d'une colonne est calculÃ© sÃ©parÃ©ment sur tous les groupes d'objets
    partageant le mÃªme identifiant de regroupement

    """

    if grouping_vect is None:
        grouping_vect = []
    if type(mat) is pd.DataFrame:
        cols = mat.columns
    else:
        cols = list(["F" + str(x) for x in range(1, mat.shape[1] + 1)])
        mat = pd.DataFrame(data=mat.values, columns=cols)
    # cols.append ('Avg_rate_by_group')

    if len(grouping_vect) == 0:
        fill_index = [0, "Avg_rate_by_feature"]
        # attribution du groupe 0 Ã  toutes les observations
        grouping_vect = pd.DataFrame(index=mat.index, columns=["Grp"])
        grouping_vect["Grp"] = 0
        # grp_values = {0}

    elif type(grouping_vect) in [pd.Series, pd.DataFrame]:
        grp_values = list(set(grouping_vect[grouping_vect.columns[0]]))
        fill_index = grp_values
        # noinspection PyTypeChecker
        fill_index.append("Avg_rate_by_feature")

    else:
        return

    df_fill_rate = pd.DataFrame(index=fill_index, columns=cols)
    df_fill_rate["Avg_rate_by_group"] = 0.0
    fill_index.remove("Avg_rate_by_feature")

    for idx in fill_index:
        for col in cols:
            subindex = grouping_vect[grouping_vect[grouping_vect.columns[0]] == idx].index
            subindex = mat.index.intersection(subindex)
            colvalues = mat.loc[subindex, col]
            colvalues = colvalues.dropna(inplace=False)
            if len(subindex) > 0:
                df_fill_rate.loc[idx, col] = float(len(colvalues) / len(subindex))

    df_fill_rate.loc["Avg_rate_by_feature", :] = np.nanmean(df_fill_rate, axis=0)
    df_fill_rate.loc[:, "Avg_rate_by_group"] = np.nanmean(df_fill_rate, axis=1)

    if len(grouping_vect) == 0:
        # pas de groupement des observations: un seul taux de remplissage par feature
        # Tri des colonnes par taux dÃ©croissant
        df_fill_rate = df_fill_rate.sort_values(by="Avg_rate_by_feature", axis=1, ascending=False, inplace=False)
    else:
        df_fill_rate = df_fill_rate.sort_values(by="Avg_rate_by_feature", axis=1, ascending=False, inplace=False)
        df_fill_rate = df_fill_rate.sort_values(by="Avg_rate_by_group", axis=0, ascending=False, inplace=False)

    return df_fill_rate.astype(float)


def sort_lines_by_fill_rate(mat):
    if type(mat) is pd.DataFrame:
        lines = mat.index
    else:
        lines = list(["I" + str(x) for x in range(1, mat.shape[0] + 1)])
        mat = pd.DataFrame(data=mat.values, index=lines)
    df_fill_rate = pd.DataFrame(index=[0], columns=lines)
    for line in lines:
        linevalues = mat.loc[line]
        linevalues = linevalues.dropna(inplace=False)
        df_fill_rate[line] = float(len(linevalues) / mat.shape[1])

    df_fill_rate = df_fill_rate.sort_values(by=0, axis=1, ascending=False)
    return df_fill_rate.astype(float)


def drop_sparse_lines(mat, minimum_fill_rate=0.5):
    if type(mat) is pd.DataFrame:
        cols = mat.columns
        lines = mat.index
    else:
        cols = list(["F" + str(x) for x in range(1, mat.shape[1] + 1)])
        lines = list(["I" + str(x) for x in range(1, mat.shape[0] + 1)])
    submat = pd.DataFrame(data=mat.values, columns=cols, index=lines)
    # df_lfill_rate = pd.DataFrame(index=[0],
    #                             columns=lines)
    lines_to_drop = []

    for line in lines:
        linevalues = mat.loc[line]
        linevalues = linevalues.dropna(inplace=False)
        if float(len(linevalues) / submat.shape[1]) < minimum_fill_rate:
            lines_to_drop.append(line)

    submat = submat.drop(lines_to_drop, axis=0)

    return submat


def drop_sparse_columns(mat, minimum_fill_rate=0.5) -> (pd.DataFrame, list):
    """

    drop_sparse_columns:
    Eliminates columns from a matrix for which the proportion of filled (not nan) values
    to the number of entries is less than a given minimum.

    Parameters
    ----------

    mat: a bi-dimensional numeric array: float[,] ndarray or DataFrame
    If mat has no index/columns, a DataFrame with automated index and columns is created.

    minimum_fill_rate: the minimum ratio. All columns with a strictly lower fill rate are dropped.

    returns
    -------

    pd.DataFrame, list: A copy of the original matrix (as DataFrame), without the sparse columns and the list of
    dropped columns
    """

    if type(mat) is pd.DataFrame:
        cols = mat.columns
        lines = mat.index
    else:
        cols = list(["F" + str(x) for x in range(1, mat.shape[1] + 1)])
        lines = list(["I" + str(x) for x in range(1, mat.shape[0] + 1)])

    submat = pd.DataFrame(data=mat.values, columns=cols, index=lines)
    # df_cfill_rate = pd.DataFrame(index=[0],
    #                             columns=cols)
    cols_to_drop = []
    for col in cols:
        colvalues = mat[col]
        colvalues = colvalues.dropna(inplace=False)
        if float(len(colvalues) / mat.shape[0]) < minimum_fill_rate:
            cols_to_drop.append(col)

    submat = submat.drop(cols_to_drop, axis=1)

    return submat, cols_to_drop


def drop_zero_columns(mat):
    """

    drop_null_columns:
    Eliminates from a matrix those columns only populated by 0.

    Parameters
    ----------

    :param mat: a bi-dimensional numeric array: float[,] ndarray or DataFrame
    If mat has no index/columns, a DataFrame with automated index and columns is created.

     :returns
    --------

    A copy of the original matrix (as DataFrame), without the null columns
    The list of dropped columns
    """

    if type(mat) is pd.DataFrame:
        cols = mat.columns
        lines = mat.index
    else:
        cols = list(["F" + str(x) for x in range(1, mat.shape[1] + 1)])
        lines = list(["I" + str(x) for x in range(1, mat.shape[0] + 1)])

    submat = pd.DataFrame(data=mat.values, columns=cols, index=lines)
    # df_cfill_rate = pd.DataFrame(index=[0],
    #                             columns=cols)
    cols_to_drop = []
    for col in cols:
        colvalues = mat[col]
        if all([v == 0 for v in colvalues]):
            cols_to_drop.append(col)

    submat = submat.drop(cols_to_drop, axis=1)

    return submat, cols_to_drop


def drop_lines_and_columns(mat, minimum_fill_rate=0.5, droplines=True, dropcols=True, colsfirst=True):
    if droplines and dropcols:
        if colsfirst:
            submat = drop_sparse_columns(mat, minimum_fill_rate)
            submat = drop_sparse_lines(submat, minimum_fill_rate)
        else:
            submat = drop_sparse_lines(mat, minimum_fill_rate)
            submat = drop_sparse_columns(submat, minimum_fill_rate)
    elif dropcols:
        submat = drop_sparse_columns(mat, minimum_fill_rate)
    else:
        submat = drop_sparse_lines(mat, minimum_fill_rate)

    return submat


def drop_features(mat, features_to_drop):
    submat = mat.copy().drop(features_to_drop, axis=1) if features_to_drop is not None else mat.copy()
    return submat


def separate_along_median_by_cols(mat):
    cols_plus = [str(x) + "_+" for x in mat.columns]
    cols_minus = [str(x) + "_-" for x in mat.columns]
    cols_meds = mat.astype(float).apply(np.nanmedian, axis=0)
    mat_plus = pd.DataFrame(index=mat.index, data=mat.values, columns=cols_plus)

    mat_minus = pd.DataFrame(index=mat.index, data=mat.values, columns=cols_minus)
    m = pd.concat([mat_plus, mat_minus], axis=1)
    for col in mat.columns:
        m[str(col) + "_+"] = np.maximum(0.0, mat[col] - cols_meds[col])
        m[str(col) + "_-"] = np.maximum(0.0, cols_meds[col] - mat[col])
    return m, cols_meds


def rebuild_median_separated_matrix(duplicated_mat, med):

    if med is None:
        pass
    [nf] = med.shape
    _, nmf = duplicated_mat.shape
    assert nmf == 2 * nf
    origin_mat = pd.DataFrame(columns=med.index, index=duplicated_mat.index, data=0)
    origin_mat = origin_mat.add(duplicated_mat.iloc[:, 0:nf].values)
    origin_mat = origin_mat.sub(duplicated_mat.iloc[:, nf: 2 * nf].values)
    origin_mat = origin_mat.T.add(med, axis="index").T
    return origin_mat


def plot_fill_rate(mat, bycols=True, grouping_vect=None, path=None):
    if grouping_vect is None:
        grouping_vect = []
    fill_rate = sort_columns_by_fill_rate(mat, grouping_vect) if bycols else sort_lines_by_fill_rate(mat)

    axis = 1 if bycols else 0

    f, ax = plt.subplots(figsize=(10, 5))
    if len(grouping_vect) == 0:
        mean_rate = np.mean(fill_rate, axis=axis)[0]
        plt.bar(
            list(range(len(fill_rate.columns))),
            fill_rate.loc[0, :], width=2.0, label="Mean {0:.1%}".format(mean_rate)
        )
        ax.set_ylabel("Fill rate")
        xlabel = "Features" if bycols else "Stocks"
        ax.set_xlabel(xlabel)
        ax.set_title(xlabel + " ordered by fill rate")
        ax.legend()
    else:
        fill_rate = fill_rate.astype(float)

        _, _ = plt.subplots(figsize=(20, 10))
        _ = sns.heatmap(
            fill_rate,
            annot=False,
            linewidths=0,
            yticklabels=True,
            xticklabels=True,
            cmap="coolwarm",
            cbar_kws={"label": "Fill rate"},
        )
    if path is not None:
        plt.gcf().savefig(path)
    plt.show()


def get_features_correlation_matrix(mat):
    mat_corr = pd.DataFrame(index=mat.columns, columns=mat.columns)
    for irow, row in enumerate(mat_corr.index):
        for icol, col in enumerate(mat_corr.columns):
            if icol == irow:
                mat_corr.iloc[irow, icol] = 1
            elif icol < irow:
                mat_corr.iloc[irow, icol] = mat_corr.iloc[icol, irow]
            else:
                mat_corr.iloc[irow, icol] = np.corrcoef(mat.iloc[:, irow], mat.iloc[:, icol])[0, 1]

    return mat_corr


def plot_values(mat, xticklabels=False, yticklabels=True, path=None):
    if not type(mat) is pd.DataFrame:
        cols = list(["F" + str(x) for x in range(1, mat.shape[1] + 1)])
        lines = list(["I" + str(x) for x in range(1, mat.shape[0] + 1)])
        mat = pd.DataFrame(index=cols, columns=lines, data=mat)
    plt.subplots(figsize=(40, 20))
    sns.heatmap(mat, annot=False, linewidths=0, xticklabels=xticklabels, yticklabels=yticklabels, cmap="bwr")

    if path is not None:
        plt.gcf().savefig(path)
    else:
        plt.show()
    plt.close("all")


def generate_random_correl_matrix(size: int, corr_max: float) -> pd.DataFrame:
    """

     generate_random_correl_matrix:
     generate a random correlation matrix having an absolute maximum correlation

     Parameters
     ----------

     size: the size of the square matrix

     corr_max: the maximum of absolute value of non-diagonal terms

     :returns:
     the desired correlation matrix as DataFrame
     """

    assert (size > 0)
    mat = np.ndarray([size, size])
    res = pd.DataFrame(mat)
    for col in range(size):
        res.iloc[col, :] = np.random.uniform(-corr_max, corr_max, size)
        res.iloc[col, col] = 1
    return res


def generate_orthogonal_vector(v: object) -> pd.DataFrame:
    """

     generate_orthogonal_vector:
     generate a random vector of the same size as a given non null vector v, orthogonal to v

     Parameters
     ----------

     v: the vector

     :returns:
     the random vector orthogonal to v
     """
    if type(v) is list:
        v = pd.DataFrame(v)
    n, _ = v.shape
    assert (n > 1)
    assert (not all([v.iloc[i, 0] == 0 for i in range(n)]))
    w = pd.DataFrame(np.zeros(n))
    if n == 2:
        w.iloc[0, 0] = v.iloc[1, 0]
        w.iloc[1, 0] = -v.iloc[0, 0]
    else:
        istar = np.argmin([1 if v.iloc[i, 0] != 0 else 0 for i in range(n)])
        w.iloc[:, 0] = np.random.uniform(-1, 1, n)
        vw = np.dot(v.T, w)[0, 0]
        w.iloc[istar, 0] = - vw / v.iloc[istar, 0]
    return w


def generate_vectors_bouquet(vpivot: Union[np.ndarray, Tuple[float], pd.DataFrame],
                             bouquet_size: int,
                             min_corr: float = 0.0,
                             positive: bool = True,
                             col_shift: int = 0,
                             row_shift: int = 0) -> pd.DataFrame:
    """
    generate_vectors_bouquet:
    generates a set of vectors around a given pivot vector, having a minimum prescribed mutual correlation
    Parameters
    ----------
    vpivot: Union[np.ndarray, Tuple[float], pd.DataFrame]
        the pivot vector used to create the bouquet
    bouquet_size: int
        number of vectors in the bouquet
    min_corr: float = 0.0
        minimum correlation between two vectors
    positive: bool = True
        if true, all vectors have positive coordinates
    col_shift: int = 0
        the integer shift applied to the column numbering
    row_shift: int = 0
        the integer shift applied to the line numbering
    Returns
    -------
    A dataframe having one line per space coordinate (common dimension), one column per vector in the bouquet
    """
    vpivot = pd.DataFrame(vpivot)
    nr, _ = vpivot.shape
    df_bouquet = pd.DataFrame(index=range(row_shift, row_shift+nr),
                              columns=range(col_shift, col_shift+bouquet_size))
    df_bouquet.fillna(value=0.0, inplace=True)
    epsilon = (1 - min_corr)**0.5
    n2v = norm_n2(vpivot)
    for icol in range(bouquet_size):
        if nr > 1:
            w = generate_orthogonal_vector(vpivot)
            n2w = norm_n2(w)
            rescaled_w = epsilon * n2v / n2w * pd.DataFrame(w)
            vals = vpivot.values + rescaled_w.values
            df_bouquet.loc[:, col_shift+icol] = vals
        else:
            vals = vpivot.values + epsilon * np.random.uniform(0, 1.0)
            df_bouquet.loc[:, col_shift+icol] = vals
    if positive:
        minval = np.min(np.min(df_bouquet))
        if minval < 0:
            df_bouquet -= minval
    return df_bouquet


def generate_clusters_centroids(ndim: int, max_corr: float, n_centroids: int = 0) -> pd.DataFrame:
    """
    Generates a vector set of fixed dimension having pairwise bounded correlation.
    By default, the set size is the same as the vector space dimension.
    TODO: handle the case where the set size is greater thar the dimension, this may result in a contradiction
    TODO: if the correlation bound is too small or null
    Parameters
    ----------
    ndim: int
        size of vectors
    max_corr: float
        maximum pairwise correlation
    n_centroids
        number of generated vectors

    Returns
    -------
    A ndim x ndim dataframe. The desired vectors are in columns and satsify the correlation condition.
    """
    assert (ndim >= 1)
    assert (0 <= max_corr < 1)
    if n_centroids == 0:
        n_centroids = ndim
    a = ndim / 4.0 * (1 - max_corr / 3)
    b = 1 - max_corr
    c = - max_corr
    delta = b*b - 4*a*c
    epsmax = 0.5 * (- b + delta**0.5) / a
    df_centroids = pd.DataFrame(index=range(ndim), columns=range(ndim))
    df_centroids.fillna(value=0.0, inplace=True)
    for icol in range(ndim):
        df_centroids.iloc[icol, icol] = 1
        df_centroids.iloc[:, icol] += np.random.uniform(0.0, 2*epsmax, ndim)

    return df_centroids


def generate_integer_partition(ntot: int, nbpart: int, nmin: int = 1):

    # assert(ntot >= nbpart*nmin)
    rng = np.random.default_rng()
    cuts = np.sort(rng.choice(ntot, nbpart, replace=False))
    res = []
    zum = 0
    defect = 0
    for i, cut in enumerate(cuts):
        diff = cut if i == 0 else cut - cuts[i-1]
        if diff < nmin:
            defect += nmin - diff
            val = nmin
        elif defect == 0:
            val = diff
        elif diff >= nmin + defect:
            val = diff - defect
            defect = 0
        else:
            val = diff
        res.append(val)
        zum += val
    if zum < ntot:
        res[-1] += ntot-zum
    # partsize = ntot / nbpart
    # res = [partsize] * nbpart
    return res


def generate_clusterized_vectors(vsize: int, nbvect: int, nclusters: int,
                                 clusterminsize: int, max_inter_corr: float,
                                 min_intra_corr: float, random_norms: bool = True) -> pd.DataFrame:
    """
    generate_clusterized_vectors: generates several bouquets or set of vectors grouped by correlation clusters.

    Parameters
    ----------
    vsize: int
        common vector length - the dimension of the surrounding vector space
    nbvect: int
        total number of vector to generate
    nclusters: int
        number of vector clusters to generate. The default case is nclusters == vsize,
        ie as many clusters as the dimension
    clusterminsize: int
        minimum size of a cluster
    max_inter_corr: float
        maximum correlation between the centroids of two distinct clusters
    min_intra_corr: float
        minimum correlation between two vectors belonging to the same cluster
    random_norms: bool
        if true, the norms of generated vectors are log-normally distributed

    Returns
    -------
    A dataframe having 'vsize' lines and 'nbvects' columns. The columns are grouped into the desired 'nclusters' groups.
    """
    clustersizes = generate_integer_partition(ntot=nbvect, nbpart=nclusters, nmin=clusterminsize)
    centroids = generate_clusters_centroids(ndim=vsize, max_corr=max_inter_corr)
    df_res = pd.DataFrame(index=range(vsize))
    # factors = np.random.exponential(size=nclusters) if random_norms else np.zeros(shape=nclusters)
    factors = np.random.lognormal(mean=0, sigma=1, size=nclusters)
    row_shift = 0
    col_shift = 0
    for icluster, csize in enumerate(clustersizes):
        if icluster < len(centroids.columns):
            centroid = centroids.iloc[:, icluster]
            df_bouquet = generate_vectors_bouquet(vpivot=centroid,
                                                  bouquet_size=int(csize),
                                                  min_corr=min_intra_corr,
                                                  col_shift=col_shift
                                                  )
            df_bouquet *= 1 + factors[icluster]

            df_res = pd.concat([df_res, df_bouquet], axis=1)
        col_shift += int(csize)
    return df_res


def build_correspondence_map(df_clusters) -> pd.DataFrame:

    assert len(df_clusters.columns) >= 2
    clusters_1 = [str(x) for x in set(df_clusters.iloc[:, 0])]
    clusters_2 = [str(x) for x in set(df_clusters.iloc[:, 1])]
    df_map = pd.DataFrame(index=clusters_1, columns=clusters_2)
    df_map.fillna(value=0.0, inplace=True)
    for idx in df_clusters.index:
        cl1 = str(df_clusters.loc[idx][0])
        cl2 = str(df_clusters.loc[idx][1])
        df_map.loc[cl1, cl2] += 1

    sum_of_cluster1 = df_map.apply(np.sum, axis=1)
    for idx in df_map.index:
        df_map.loc[idx, :] /= sum_of_cluster1[idx]

    return df_map


def plot_correspondence_map(df_clusters, path=None):

    df_map = build_correspondence_map(df_clusters)
    plot_values(df_map, xticklabels=True, path=path)
    return None


def duplicate_data_wfold(df: pd.DataFrame, window: int) -> pd.DataFrame:
    if window <= 0:
        return pd.DataFrame()
    cols = df.columns
    nidx = len(df.index)
    l_shifted_cols = [[f"{c}_{i}" for i in range(window)] for c in cols]
    shifted_cols = [item for sublist in l_shifted_cols for item in sublist]
    duplicated_df = pd.DataFrame(index=df.index, columns=shifted_cols)
    for irow, idx in enumerate(df.index):
        for icol, col in enumerate(cols):
            for shift in range(window):
                duplicated_df.loc[idx, f"{col}_{shift}"] = df.iloc[irow+shift, icol] if irow+shift < nidx else np.nan
    return duplicated_df

